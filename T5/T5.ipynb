{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective Training\n",
    "\n",
    "Objectives: study how to debug backprop using numerical gradient verification, experiment with parameters initialisation and combat overfitting with regularisation.\n",
    "\n",
    "> **Instructions:** ensure your Python environment is setup with the following additional packages: \n",
    ">\n",
    "> - `t5utils.py` contains unit tests to check your code and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import t5utils as t5\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Gradient Verification ##\n",
    "\n",
    "The backprop algorithm for training deep neural networks requires the computation of $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$, the gradient of a loss function $\\mathcal{L}$ with respect to all model parameters $\\theta$. The loss is computed using forward propagation.\n",
    "\n",
    "By definition the partial derivative of $\\mathcal{L}$ with respect to $\\mathcal{\\theta}$ is:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{\\mathcal{L}(\\theta + \\varepsilon) - \\mathcal{L}(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "Assuming that our inference model implementation is correct (forward prop), we can check the gradients computation by evaluating $\\mathcal{L}(\\theta + \\varepsilon)$ and $\\mathcal{L}(\\theta - \\varepsilon)$ using forward prop for a small value of $\\varepsilon$.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 1:**</font> Complete the gradient verification function for a simple linear model where $\\theta = W$.\n",
    ">\n",
    "> - Implement the linear forward propagation\n",
    "> - Implement the linear backprop\n",
    "> - Implement the gradient verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation for linear model\n",
    "def linear_fwd(W, X):\n",
    "    \"\"\"\n",
    "    Linearity forward\n",
    "\n",
    "    Arguments:\n",
    "    W -- weights, (n_x, 1)\n",
    "    X -- input (n, n_x)\n",
    "\n",
    "    Returns:\n",
    "    Y -- linear output, shape (n, 1)\n",
    "    cache -- dictionary for backpropagation\n",
    "        W -- weights, (n_x, 1)\n",
    "        X -- linear input, (n, n_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    Y = W.T @ X.T\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    cache = {'W': W, 'X': X}\n",
    "\n",
    "    return Y, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = [[0.772]]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "np.random.seed(2019)\n",
    "Y, _ = linear_fwd(np.random.randn(2,1), np.random.randn(1,2))\n",
    "print(\"Y = {}\".format(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `Y = [[0.772]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation for linearity\n",
    "def linear_back(L, cache):\n",
    "    \"\"\"\n",
    "    Linearity backprop\n",
    "\n",
    "    Arguments:\n",
    "    L -- loss\n",
    "    cache -- dictionary from forward propagation\n",
    "        W -- weights, (n_x, 1)\n",
    "        X -- linear input, (n, n_x)\n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of L with respect to W, (n_x, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    X = cache[\"X\"]\n",
    "    n = X.shape[0]\n",
    "    dW = X.T/n\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "        \n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW.T = [[1.481 1.332]]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "np.random.seed(2019)\n",
    "W, X = (np.random.randn(2,1), np.random.randn(1,2))\n",
    "L, cache = linear_fwd(W, X)\n",
    "dW = linear_back(L, cache)\n",
    "print(\"dW.T = {}\".format(dW.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `dW.T = [[1.481 1.332]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient verification\n",
    "def check_linear_grads(W, X, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Gradient verification for simple linear function\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, (n_x, 1)\n",
    "    X -- input (n, n_x)\n",
    "    epsilon -- small scalar e.g. 1e-5\n",
    "    \n",
    "    Returns:\n",
    "    diff -- difference between gradient approximation and backprop evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    L, cache = linear_fwd(W, X)\n",
    "    grad = linear_back(L, cache)\n",
    "\n",
    "    grad_approx = np.zeros(grad.shape)\n",
    "    for i in range(np.prod(W.shape)):\n",
    "        W_plus = W.copy()\n",
    "        W_minus = W.copy()        \n",
    "        ### INPUT YOUR CODE HERE ### (5 lines)\n",
    "        W_plus[i] =  L + epsilon\n",
    "        W_minus[i] = L - epsilon\n",
    "        L_plus, _ = linear_fwd(W_plus[i], X)\n",
    "        L_minus, _ = linear_fwd(W_minus[i], X)\n",
    "        grad_approx[i] = (L_plus[i] - L_minus[i])/2@epsilon\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "   \n",
    "    diff = np.linalg.norm(grad - grad_approx)\n",
    "    diff = diff / (np.linalg.norm(grad) + np.linalg.norm(grad_approx))  \n",
    "    if diff > 1e-8:\n",
    "        print(\"Gradient Implementation Error\")\n",
    "    \n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-644c1ef1105f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_linear_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient diff = {:.3e}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-acce7e848bee>\u001b[0m in \u001b[0;36mcheck_linear_grads\u001b[0;34m(W, X, epsilon)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mW_plus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mW_minus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mL_plus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_plus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mL_minus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_minus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mgrad_approx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mL_plus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mL_minus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f6b3ae95a73a>\u001b[0m in \u001b[0;36mlinear_fwd\u001b[0;34m(W, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m### INPUT YOUR CODE HERE ### (1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m### END OF YOUR CODE SEGMENT ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "np.random.seed(2019)\n",
    "W, X = (np.random.randn(2,1), np.random.randn(1,2))\n",
    "diff = check_linear_grads(W, X)\n",
    "print(\"gradient diff = {:.3e}\".format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `gradient diff = 1.637e-12`\n",
    "\n",
    "With a relative difference of less than 1e-10 between the numerical gradient approximation and the gradient implementation, you can be confident that your code is correct. \n",
    "\n",
    "***\n",
    "\n",
    "Implement the gradient verification for the deep feedforward neural network implemented in the previous notebook and whose backprop is depicted below: \n",
    "\n",
    "<img src=\"figs/backprop.png\" style=\"width:600px;\">\n",
    "\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 2:**</font> Complete the gradient verification function for $K$ layers where the model parameters are $\\theta \\equiv (W^{[1]}, b^{[1]},\\ldots, W^{[K]}, b^{[K]})$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grads(params, grads, X, Y, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Gradient verification for deep neural network\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing the model parameters\n",
    "    grads -- dictionary with gradients\n",
    "    X -- data sample\n",
    "    Y -- ground truth label\n",
    "    epsilon -- small scalar\n",
    "    \n",
    "    Returns:\n",
    "    diff -- difference between gradient appoximation and backprop evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    theta = t5.params2theta(params) # convert params to list of scalars\n",
    "    grad_approx = np.zeros(theta.shape)\n",
    "    for i in range(theta.shape[-1]):\n",
    "        theta_plus = theta.copy()\n",
    "        theta_minus = theta.copy()\n",
    "        \n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        theta_plus[i] = None\n",
    "        theta_minus[i] = None\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "        \n",
    "        # convert list of scalars to params\n",
    "        params_plus = t5.theta2params(theta_plus, params) \n",
    "        params_minus = t5.theta2params(theta_minus, params)\n",
    "        \n",
    "        _, L_plus, _ = t5.forward_prop(params_plus, X, Y)\n",
    "        _, L_minus, _ = t5.forward_prop(params_minus, X, Y)\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        grad_approx[i] = None\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "\n",
    "    grad_flat = np.array([])\n",
    "    for key in params.keys():\n",
    "        grad_flat = np.concatenate((grad_flat, grads['d'+key].reshape(-1)))\n",
    "\n",
    "    diff = np.linalg.norm(grad_flat - grad_approx)\n",
    "    diff = diff / (np.linalg.norm(grad_flat) + np.linalg.norm(grad_approx))  \n",
    "    \n",
    "    if diff > 1e-7:\n",
    "        print(\"Gradient Implementation Error\")\n",
    "    \n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "X, Y, params = t5.forward_prop_test()\n",
    "A, loss, caches = t5.forward_prop(params, X, Y)\n",
    "grads = t5.back_prop(A, Y, caches)\n",
    "diff = check_grads(params, grads, X, Y)\n",
    "print(\"gradient diff = {:.3e}\".format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `gradient diff = 1.243e-10`\n",
    "\n",
    "We must compute the gradient approximation $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_i}$ for all the scalars of the weight matrices and bias vectors. As a result, gradient checking is computationally expensive and slow. It is generally carried out on one data sample and for just a few iterations of the backprop algorithm. It is good practice to carry out a gradient check before training a new model.\n",
    "\n",
    "## B. Parameter Initialisation ##\n",
    "\n",
    "\n",
    "Initialisation of model parameters is an important initial step in the training process that may lead to different optimisation results for the same model topology. A well chosen initialisation can:\n",
    "\n",
    "- lead to improved generalisation performance, reaching lower loss\n",
    "- speed-up convergence of the gradient descent\n",
    "\n",
    "Let's experiment with a 3-layer neural network [LINEAR+ReLU, 20 units]->[LINEAR+ReLU, 8 units]->[LINEAR+Sigmoid, 1 unit] using a cross-entropy loss as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter optimisation using backprop\n",
    "def model_fit(X, Y, n_h=[20, 8], epochs=15000, learning_rate=0.01, verbose=False, init='he'):\n",
    "    \"\"\"\n",
    "    Optimise model parameters by performing gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    n_h -- array with number of units in hidden layers, size K-1\n",
    "    X -- n data samples  (n, n_x)\n",
    "    Y -- groud truth label vector of size (n, n_y)\n",
    "    epochs -- number of iteration updates through dataset\n",
    "    learning_rate -- learning rate of the gradient descent\n",
    "    init -- string, either 'random', 'zeros' or 'he'\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing model parameters\n",
    "    grads -- dictionary with final gradients\n",
    "    loss_log -- list of loss values for every 100 updates\n",
    "    \"\"\"\n",
    "    \n",
    "    # returns array [n_x, n_h[0], ..., n_h[K-1], n_y]\n",
    "    dims = t5.model_config(X, Y, n_h)\n",
    "\n",
    "    # create and initialise model parameters\n",
    "    if init == 'random':\n",
    "        params = random_init(dims)\n",
    "    elif init == 'zeros':\n",
    "        params = zeros_init(dims)\n",
    "    elif init == 'he':\n",
    "        params = he_init(dims)\n",
    "    \n",
    "    loss_log = []\n",
    "    for i in range(epochs):\n",
    "        A, loss, caches = t5.forward_prop(params, X, Y) # Cost and gradient computation\n",
    "        grads = t5.back_prop(A, Y, caches)\n",
    "        params = t5.update_params(params, grads, learning_rate)        \n",
    "        \n",
    "        # logs\n",
    "        if i % 100 == 0:\n",
    "            loss_log.append(loss.item())\n",
    "            if verbose:\n",
    "                print(\"Loss after {} epochs: {:.3f}\".format(i, loss))\n",
    "     \n",
    "    return params, grads, loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start by creating a simple dataset from Scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "\n",
    "# training set\n",
    "X_train, Y_train = sklearn.datasets.make_moons(n_samples=512, noise=.8)\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "# test set\n",
    "X_test, Y_test = sklearn.datasets.make_moons(n_samples=256, noise=.8)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train.reshape(-1), s=20, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialise the model parameters $\\theta \\equiv (W^{[1]}, b^{[1]},\\ldots, W^{[K]}, b^{[K]})$ to zeros. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 3:**</font> Complete the following function using `np.zeros(shape)` where `shape` is the shape of the numpy array to be initialised with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_init(dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dims -- array with number of units in each layer, size K\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing initialised model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised bias vector of shape (1, n_h1)\n",
    "        ...\n",
    "        Wk -- initialised weight matrix of shape (n_hk-1, n_hk)w\n",
    "        bk -- initialised bias vector of shape (1, n_hk)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    K = len(dims) # number of layers\n",
    "\n",
    "    params = {}    \n",
    "    for k in range(1, K):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        params['W{}'.format(k)] = None\n",
    "        params['b{}'.format(k)] = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "                                                    \n",
    "        assert(params['W{}'.format(k)].shape == (dims[k - 1], dims[k]))\n",
    "        assert(params['b{}'.format(k)].shape == (1, dims[k]))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "np.random.seed(2019)\n",
    "params = zeros_init([3,2,1])\n",
    "print(\"W1 = {}\".format(params[\"W1\"]))\n",
    "print(\"b1 = {} \".format(params[\"b1\"]))\n",
    "print(\"W2 = {}\".format(params[\"W2\"]))\n",
    "print(\"b2 = {}\".format(params[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `W1 = [[0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]]\n",
    "b1 = [[0. 0.]] \n",
    "W2 = [[0.]\n",
    " [0.]]\n",
    "b2 = [[0.]]`\n",
    "\n",
    "***\n",
    "\n",
    "Evaluate the following cell to train the model on the dataset, initialising all parameters to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads, loss_log = model_fit(X_train, Y_train, init='zeros')\n",
    "print(\"loss after {} epochs = {:.2f}\".format(len(loss_log) * 100, loss_log[-1]))\n",
    "\n",
    "Y_hat_train = t5.model_predict(params, X_train)\n",
    "Y_hat_test = t5.model_predict(params, X_test)\n",
    "train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
    "test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))\n",
    "print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "\n",
    "t5.plot_model(lambda x: t5.model_predict(params, x), X_test, Y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training failed and model did not learn anything, predicting 0 for every examples. By initialising all parameters to 0, the network units are learning the same thing and we may as well just keep a single unit and train a linear classifier like the logistic regression model. To get some meaningful prediction from our model, we must break the network symmetry and initialise our parameters from $\\mathcal{N}(0,\\sigma^2)$.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 4:**</font> Complete the following function, initialising weights $\\sim\\mathcal{N}(0,3)$ and setting biases to zeros. Use `np.random.randn(n1, n2)` to generate a random matrix of shape $(n_1,n_2)$ $\\sim\\mathcal{N}(0,1)$ (zero mean, unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init(dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dims -- array with number of units in each layer, size K\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing initialised model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised bias vector of shape (1, n_h1)\n",
    "        ...\n",
    "        Wk -- initialised weight matrix of shape (n_hk-1, n_hk)\n",
    "        bk -- initialised bias vector of shape (1, n_hk)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    K = len(dims) # number of layers\n",
    "\n",
    "    params = {}    \n",
    "    for k in range(1, K):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        params['W{}'.format(k)] = None\n",
    "        params['b{}'.format(k)] = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "                                                    \n",
    "        assert(params['W{}'.format(k)].shape == (dims[k - 1], dims[k]))\n",
    "        assert(params['b{}'.format(k)].shape == (1, dims[k]))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "np.random.seed(2019)\n",
    "params = random_init([3,2,1])\n",
    "print(\"W1 = {}\".format(params[\"W1\"]))\n",
    "print(\"b1 = {}\".format(params[\"b1\"]))\n",
    "print(\"W2 = {}\".format(params[\"W2\"]))\n",
    "print(\"b2 = {}\".format(params[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `W1 = [[-0.653  2.464]\n",
    " [ 4.444  3.996]\n",
    " [-1.086  2.057]]\n",
    "b1 = [[0. 0.]]\n",
    "W2 = [[1.721]\n",
    " [0.863]]\n",
    "b2 = [[0.]]`\n",
    "\n",
    "***\n",
    "\n",
    "Evaluate the following cell to train the model on the same dataset, initialising parameters with \"large\" random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "params, grads, loss_log = model_fit(X_train, Y_train, init='random')\n",
    "print(\"loss after {} epochs = {:.2f}\".format(len(loss_log) * 100, loss_log[-1]))\n",
    "\n",
    "plt.plot(loss_log)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.show()\n",
    "\n",
    "Y_hat_train = t5.model_predict(params, X_train)\n",
    "Y_hat_test = t5.model_predict(params, X_test)\n",
    "train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
    "test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))\n",
    "print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "\n",
    "t5.plot_model(lambda x: t5.model_predict(params, x), X_test, Y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is improved, however large random weights can result in a large input value to the final sigmoid activation. As a result, the network will have results close to 0 or 1 and the cross-entropy loss will be very large. This is known as poor initialisation and can lead to vanishing/exploding gradients. This also slows down the optimisation algorithm. Training the network longer may improve results.\n",
    "\n",
    "A better strategy is to adapt the variance of the weights's distribution to the network topology. This is known as the **Xavier initialisation** where the scaling factor is $\\sqrt{{1}\\over{n_{k-1}}}$ for $W^{[k]}$. An initialisation variation with improved performance for the ReLU activation was subsequently proposed by He et al. (2015) by scaling the Xavier initialisation: $\\sqrt{{2}\\over{n_{k-1}}}$.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 5:**</font> Complete the following function, initialising weights with He-scaling and setting biases to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_init(dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dims -- array with number of units in each layer, size K\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing initialised model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised bias vector of shape (1, n_h1)\n",
    "        ...\n",
    "        Wk -- initialised weight matrix of shape (n_hk-1, n_hk)\n",
    "        bk -- initialised bias vector of shape (1, n_hk)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    K = len(dims) # number of layers\n",
    "\n",
    "    params = {}    \n",
    "    for k in range(1, K):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        params['W{}'.format(k)] = None\n",
    "        params['b{}'.format(k)] = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "                                                    \n",
    "        assert(params['W{}'.format(k)].shape == (dims[k - 1], dims[k]))\n",
    "        assert(params['b{}'.format(k)].shape == (1, dims[k]))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "np.random.seed(2019)\n",
    "params = he_init([3,2,1])\n",
    "print(\"W1 = {}\".format(params[\"W1\"]))\n",
    "print(\"b1 = {}\".format(params[\"b1\"]))\n",
    "print(\"W2 = {}\".format(params[\"W2\"]))\n",
    "print(\"b2 = {}\".format(params[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `W1 = [[-0.178  0.671]\n",
    " [ 1.209  1.087]\n",
    " [-0.295  0.56 ]]\n",
    "b1 = [[0. 0.]] \n",
    "W2 = [[0.574]\n",
    " [0.288]]\n",
    "b2 = [[0.]]`\n",
    "\n",
    "***\n",
    "\n",
    "Evaluate the following cell to train the model on the same dataset, initialising parameters with the He initialisation technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "params, grads, loss_log = model_fit(X_train, Y_train, init='he')\n",
    "print(\"loss after {} epochs = {:.2f}\".format(len(loss_log) * 100, loss_log[-1]))\n",
    "\n",
    "plt.plot(loss_log)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.show()\n",
    "\n",
    "Y_hat_train = t5.model_predict(params, X_train)\n",
    "Y_hat_test = t5.model_predict(params, X_test)\n",
    "train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
    "test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))\n",
    "print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "\n",
    "t5.plot_model(lambda x: t5.model_predict(params, x), X_test, Y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've improved our test accuracy by almost 5%. The **He initialisation** performs well and is a good choice in practice.\n",
    "\n",
    "## C. Regularisation ##\n",
    "\n",
    "Deep networks with large capacity can learn complex representations, however they are often prone to overfitting particularly if the supervised dataset is modest in size. Overfitting results in poor generalisation capabilities for the trained model. \n",
    "\n",
    "Evaluate the following cell to train a deep network with large capacity (this may take a few minutes to complete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "params, grads, loss_log = model_fit(X_train, Y_train, n_h=[18, 9, 9], epochs=30000, learning_rate=0.1, verbose=False, init='he')\n",
    "print(\"loss after {} epochs = {:.2f}\".format(len(loss_log) * 100, loss_log[-1]))\n",
    "\n",
    "plt.plot(loss_log)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.show()\n",
    "\n",
    "Y_hat_train = t5.model_predict(params, X_train)\n",
    "Y_hat_test = t5.model_predict(params, X_test)\n",
    "train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
    "test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))\n",
    "print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "\n",
    "t5.plot_model(lambda x: t5.model_predict(params, x), X_test, Y_test.reshape(-1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-regularised model is overfitting the training set. It is fitting the dataset noise, leading to suboptimal generalisation.\n",
    "\n",
    "### C.1 $L_2$ Regularisation ###\n",
    "\n",
    "A common technique to reduce overfitting is called $L_2$ regularisation. It entails penalising the loss with the Frobenius norm of the parameter weight matrices. If $\\mathcal{L}$ is the cross-entropy loss, the $L_2$ regularised loss is given by:\n",
    "\n",
    "\n",
    "$$\\mathcal{L}_{reg} = \\overbrace{-\\frac{1}{n}\\left(Y^T\\, \\log A^{[K]} + (1-Y)^T\\,\\log(1-A^{[K]})\n",
    "\\right)}^\\text{cross-entropy loss} + \\overbrace{\\frac{1}{n}\\frac{\\lambda}{2}\\left( \\sum_k \\left\\|W^{[k]}\\right\\|^2_F \\right)}^\\text{$L_2$ regularisation} \\tag{2}$$\n",
    "\n",
    "$L_2$ regularisation is parametrised by the scalar $\\lambda$. We must now modify our training function.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 6:**</font> Complete the following function, adding a $L_2$ regularisation term for all weights matrices to the cross-entropy loss computed during forward propagation. Note $\\left\\|W^{[k]}\\right\\|^2_F$ is simply computed in numpy with `np.sum(np.square(Wk))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter optimisation using regularisation\n",
    "def regularised_model_fit(X, Y, n_h, lambd, epochs, learning_rate, verbose=False, init=\"he\"):\n",
    "    \"\"\"\n",
    "    Optimise model parameters by performing gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    n_h -- array with number of units in hidden layers, size K-1\n",
    "    X -- n data samples (n, n_x)\n",
    "    Y -- ground truth label vector of size (n, n_y)\n",
    "    lambd -- regularisation scalar\n",
    "    epochs -- number of iteration updates through dataset\n",
    "    learning_rate -- learning rate of the gradient descent\n",
    "    init -- string, either 'random', 'zeros' or 'he'\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing model parameters\n",
    "    grads -- dictionary with final gradients\n",
    "    loss_log -- list of loss values for every 100 updates\n",
    "    \"\"\"\n",
    "    \n",
    "    # returns array [n_x, n_h[0], ..., n_h[K-1], n_y]\n",
    "    dims = t5.model_config(X, Y, n_h)\n",
    "\n",
    "    # create and initialise model parameters\n",
    "    if init == 'random':\n",
    "        params = random_init(dims)\n",
    "    elif init == 'zeros':\n",
    "        params = zeros_init(dims)\n",
    "    elif init == 'he':\n",
    "        params = he_init(dims)\n",
    "    \n",
    "    loss_log = []\n",
    "    K = len(params) >> 1\n",
    "    n = Y.shape[0]\n",
    "    for i in range(epochs):\n",
    "        A, loss, caches = t5.forward_prop(params, X, Y)\n",
    "        \n",
    "        for k in range(1, K):  \n",
    "            ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "            Wk = None\n",
    "            loss = None \n",
    "            ### END OF YOUR CODE SEGMENT ###  \n",
    "        grads = regularised_back_prop(A, Y, caches, lambd)\n",
    "        params = t5.update_params(params, grads, learning_rate)        \n",
    "        \n",
    "        # logs\n",
    "        if i % 100 == 0:\n",
    "            loss_log.append(loss.item())\n",
    "            if verbose:\n",
    "                print(\"Loss after {} epochs: {:.3f}\".format(i, loss))\n",
    "     \n",
    "    return params, grads, loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an extra term in the loss expression in equation (2), we must also update the backprop functions and recompute the gradients. Going through the differential computation of our network topology, we can quickly see that the regularisation term only affect the linear backprop with an extra component added to $dW^{[k]}$ given by $\\frac{\\partial }{\\partial W^{[k]}} \\left( \\frac{1}{n} \\frac{\\lambda}{2} \\left\\|W^{[k]}\\right\\|^2_F\\right ) = \\frac{\\lambda}{n} W^{[k]}$.\n",
    "\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 7:**</font>  Implement the new linear backward propagation function to take into account regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularised_back_prop(AK, Y, caches, lambd):\n",
    "    \"\"\"\n",
    "    Compute back-propagation gradients with regularisation\n",
    "    \n",
    "    Arguments:\n",
    "    AK -- probability vector, final layer output, shape (1, n_y)\n",
    "    Y -- ground truth output (n, n_y)\n",
    "    caches -- array of layer cache, len=K\n",
    "    lambd -- regularisation scalar\n",
    "\n",
    "    Returns:\n",
    "    grads -- dictionary containing your gradients with respect to all parameters\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    K = len(caches)\n",
    "    n = AK.shape[0]\n",
    "    cache = caches[K - 1]\n",
    "    \n",
    "    # stable backprop for sigmoid output layer\n",
    "    dZK = AK - Y\n",
    "    grads[\"dW{}\".format(K)], grads[\"db{}\".format(K)], grads[\"dA{}\".format(K)] = regularised_linear_back(dZK, cache['LINEAR'], lambd)\n",
    "\n",
    "    for k in reversed(range(K - 1)):\n",
    "        cache = caches[k]\n",
    "        grads[\"dW{}\".format(k + 1)], grads[\"db{}\".format(k + 1)], grads[\"dA{}\".format(k + 1)] = regularised_singlelayer_back(grads[\"dA{}\".format(k + 2)], cache, 'ReLU', lambd)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularised_singlelayer_back(dA, cache, non_linearity='ReLU', lambd=0):\n",
    "    \"\"\"\n",
    "    Single layer backprop (linear + non-linearity) with regularisation\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        LINEAR -- dictionary from forward linear propagation \n",
    "        ACTIVATION -- dictionary from forward non-linearity propagation \n",
    "    non_linearity -- string ('ReLU' or 'Sigmoid') activation for layer\n",
    "    lambd -- regularisation scalar\n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of loss with respect to current layer weights\n",
    "    db -- gradient of loss with respect to current layer bias\n",
    "    dA_prev -- gradient of loss with respect to activation of previous layer output\n",
    "    \"\"\"\n",
    "        \n",
    "    linear_cache = cache['LINEAR']\n",
    "    activation_cache = cache['ACTIVATION']\n",
    "    if non_linearity == 'Sigmoid':\n",
    "        dZ = t5.sigmoid_back(dA, activation_cache)\n",
    "    elif non_linearity == 'ReLU':\n",
    "        dZ = t5.relu_back(dA, activation_cache)\n",
    "    dW, db, dA_prev = regularised_linear_back(dZ, linear_cache, lambd)\n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularised_linear_back(dZ, cache, lambd):\n",
    "    \"\"\"\n",
    "    Linearity backprop\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- gradient of loss with respect to current layer linear output\n",
    "    cache -- dictionary from forward propagation\n",
    "        W -- weight matrix\n",
    "        b -- bias row vector\n",
    "        A_prev -- previous layer activation input\n",
    "    lambd -- regularisation scalar\n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of loss with respect to current layer weights\n",
    "    db -- gradient of loss with respect to current layer bias\n",
    "    dA_prev -- gradient of loss with respect to activation of previous layer output\n",
    "    \"\"\"\n",
    "    dW, db, dA_prev = t5.linear_back(dZ, cache)\n",
    "    ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "    A_prev = None\n",
    "    W = None\n",
    "    n = None\n",
    "    dW = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "AK, Y, caches = t5.back_prop_test()\n",
    "grads = regularised_back_prop(AK, Y, caches, 0.7)\n",
    "print(\"dW1.T = {}\".format(grads['dW1'].T))\n",
    "print(\"dW2.T = {}\".format(grads['dW2'].T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `dW1.T = [[-1.121 -0.656  1.065 -0.421]\n",
    " [-1.08  -0.777  0.397 -0.781]\n",
    " [-0.208 -0.259  0.348 -0.077]]\n",
    "dW2.T = [[-0.003 -0.66  -0.457]]`\n",
    "\n",
    "***\n",
    "\n",
    "Evaluate the cell below and observe the effect of the $L_2$ regularisation on the model training setting the regularisation hyperparameter scalar to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "params, grads, loss_log = regularised_model_fit(X_train, Y_train, lambd=1.0, n_h=[18, 9, 9], epochs=30000, learning_rate=0.1, verbose=False, init='he')\n",
    "\n",
    "plt.plot(loss_log)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.show()\n",
    "\n",
    "print(\"loss after {} epochs = {:.2f}\".format(len(loss_log) *100, loss_log[-1]))\n",
    "Y_hat_train = t5.model_predict(params, X_train)\n",
    "Y_hat_test = t5.model_predict(params, X_test)\n",
    "train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
    "test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))\n",
    "print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "\n",
    "t5.plot_model(lambda x: t5.model_predict(params, x), X_test, Y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_2$ regularisation enforce small weights on our trained model parameters by penalising the square values of the weights in the regularised loss function (large weight values become prohibitive in loss).\n",
    "\n",
    "With the same network topology, we now have a model that generalise better on the test dataset, resolving the overfitting issue observed previously. $L_2$ regularisation requires to select a value for hyper-parameter $\\lambda$. This is generally carried out on a separate validation set. $L_2$ regularisation has for effect to smooth the decision boundaries, however setting $\\lambda$ to a really large value can result in a high bias model.\n",
    "\n",
    "### C.2 Dropout ###\n",
    "\n",
    "Another regularisation technique widely employed in deep learning is called dropout. It consists in randomly shutting down some neurons in each iteration (epoch or minibatch) of the training process.\n",
    "\n",
    "The idea behind drop-out is to train a different model at each iteration, only using a subset of the layers' units. \n",
    "As a result, neuron units become less sensitive to the activation of specific units from the previous layer (other units can be shut down at any time). Dropout modifies the model at each training iteration.\n",
    "\n",
    "Dropout is specified at each layer level by setting a dropout rate (probability) in the range of (0,1) that corresponds to the percentage of units randomly turned off during each training iteration. This means both forward and back propagation must be modified to support dropout.\n",
    "\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 8:**</font> Modify the feedforward function, adding dropout to the hidden layer activations. For $k\\in \\{1,\\ldots,K-1\\}$ (no dropout for output layer), let $p_k=$`dropouts[k-1]`\n",
    ">\n",
    "> - create a random matrix $D^{[k]}$ (same shape as $A^{[k]}$ activations), drawn from the uniform distribution (range 0 to 1)\n",
    "> - set entries of $D^{[k]}$ to 0 with probability in $p_k$ and to 1 with probability $1-p_k$ \n",
    "> - shut down neuron activation using dropout random matrix i.e. set $A^{[k]} \\leftarrow A^{[k]} \\odot D^{[k]}$\n",
    "> - scale all $A^{[k]}$ activation entries by $\\frac{1}{1-p_k}$ so that to keep the same expected value for the loss (this is known as the inverted dropout)\n",
    "> - add the dropout matrix $D^{[k]}$ to the cache for backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_with_dropout(params, X, Y=None, dropouts=None):\n",
    "    \"\"\"\n",
    "    Compute the layer activations and loss if needed\n",
    "\n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- optional argument, ground truth label, column vector of shape (n, n_y)\n",
    "    dropouts -- optional argument, list of dropout rates from A1 to AK-1\n",
    "\n",
    "    Returns:\n",
    "    A -- final layer output (activation value) \n",
    "    loss -- cross-entropy loss or NaN if Y=None\n",
    "    caches -- array of caches for the K layers\n",
    "    \"\"\"\n",
    "        \n",
    "    caches = []\n",
    "    K = len(params) >> 1\n",
    "    A = X\n",
    "    for k in range(1, K):\n",
    "        A_prev = A\n",
    "        W = params['W{}'.format(k)]\n",
    "        b = params['b{}'.format(k)]\n",
    "        A, cache = t5.singlelayer_fwd(W, b, A_prev, non_linearity='ReLU')\n",
    "\n",
    "        if dropouts is not None:\n",
    "            dropout_rate = dropouts[k - 1]\n",
    "            ### INPUT YOUR CODE HERE ### (5 lines)\n",
    "            D = None    # initialize dropout matrix D = np.random.rand(..., ...)\n",
    "            D = None    # convert entries of D to 0 or 1\n",
    "            A = None    # shut down neuron units of A\n",
    "            A = None    # scale value of active neurons units\n",
    "            cache['DROPOUT'] = None    # add dropout matrix to cache\n",
    "            ### END OF YOUR CODE SEGMENT ###\n",
    "        \n",
    "        caches.append(cache)\n",
    "        \n",
    "    A_prev = A\n",
    "    W = params['W{}'.format(K)]\n",
    "    b = params['b{}'.format(K)]\n",
    "    A, cache = t5.singlelayer_fwd(W, b, A_prev, non_linearity='Sigmoid')\n",
    "    \n",
    "    caches.append(cache)\n",
    "    loss = float('nan')\n",
    "    if Y is not None:\n",
    "        n = Y.shape[0]\n",
    "        loss = np.multiply(-np.log(A),Y) + np.multiply(-np.log(1 - A), 1 - Y)\n",
    "        loss = np.squeeze(np.nansum(loss) / n)\n",
    "    return A, loss, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "X, Y, params, dropouts = t5.forward_prop_with_dropout_test()\n",
    "A, loss, caches = forward_prop_with_dropout(params, X, Y, dropouts)\n",
    "for i, cache in enumerate(caches[0:len(dropouts)]):\n",
    "        print(\"Layer {} dropout = {:.2f} (target {:.2f})\".format(i + 1, 1 - np.mean(cache['DROPOUT']), dropouts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `Layer 1 dropout = 0.53 (target 0.50)\n",
    "Layer 2 dropout = 0.43 (target 0.50)\n",
    "Layer 3 dropout = 0.28 (target 0.30)`\n",
    "\n",
    "***\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 9:**</font> Implement the dropout backprop function using the random  dropout matrix $D^{[k]}$ computed during forward propagation and stored in the cache. Remember that we have no dropout on the output layer.\n",
    "> \n",
    "> - set $\\frac{\\partial \\mathcal{L}}{\\partial A^{[k]}}=0$ for the neurons shutdown during forward propagation\n",
    "> - scale $\\frac{\\partial \\mathcal{L}}{\\partial A^{[k]}}$ by same $\\frac{1}{1-p_k}$ to match the forward propagation scaling of the neuron activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_with_dropout(AK, Y, caches, dropouts=None):\n",
    "    \"\"\"\n",
    "    Compute back-propagation gradients with dropout\n",
    "    \n",
    "    Arguments:\n",
    "    AK -- probability vector, final layer output, shape (1, n_y)\n",
    "    Y -- ground truth output (n, n_y)\n",
    "    caches -- array of layer cache, len=K\n",
    "    dropouts -- optional argument, list of dropout rates (K-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- dictionary containing your gradients with respect to all parameters\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    if dropouts is not None:\n",
    "        dropout_cache = {}\n",
    "        # retrieve dropout cache\n",
    "        for i, cache in enumerate(caches[0:len(dropouts)]): # no dropout on output layer\n",
    "            dropout_cache[\"D{}\".format(i + 1)] = cache['DROPOUT']\n",
    "        \n",
    "    grads = {}\n",
    "    K = len(caches)\n",
    "    n = AK.shape[0]\n",
    "    cache = caches[K - 1]\n",
    "    # stable backprop for sigmoid output layer\n",
    "    dZK = AK - Y\n",
    "    grads[\"dW{}\".format(K)], grads[\"db{}\".format(K)], grads[\"dA{}\".format(K)] = t5.linear_back(dZK, cache['LINEAR'])\n",
    "    \n",
    "    for k in reversed(range(K - 1)):\n",
    "        cache = caches[k]\n",
    "        dA = grads[\"dA{}\".format(k + 2)]\n",
    "                \n",
    "        # Hidden layer dropout\n",
    "        if dropouts is not None:\n",
    "            dropout_rate = dropouts[k]\n",
    "            ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "            dA = None    # shut down the same units as during forward propagation\n",
    "            dA = None    # scale value of active neuron units\n",
    "            ### END OF YOUR CODE SEGMENT ###\n",
    "        \n",
    "        grads[\"dW{}\".format(k + 1)], grads[\"db{}\".format(k + 1)], grads[\"dA{}\".format(k + 1)] = t5.singlelayer_back(dA, cache, 'ReLU')\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "X, Y, params, dropouts = t5.backprop_with_dropout_test()\n",
    "A, loss, caches = forward_prop_with_dropout(params, X, Y, dropouts)\n",
    "grads = back_prop_with_dropout(A, Y, caches, dropouts)\n",
    "print(\"dW1.T = {}\".format(grads['dW1'].T))\n",
    "print(\"db1 = {}\".format(grads['db1']))\n",
    "print(\"dA1 = {}\".format(grads['dA1']))\n",
    "print(\"dW2.T = {}\".format(grads['dW2'].T))\n",
    "print(\"db2 = {}\".format(grads['db2']))\n",
    "print(\"dA2 = {}\".format(grads['dA2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `dW1.T = [[ 0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
    " [ 0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
    " [ 2.915 11.002 19.838 17.837  4.846  9.182  7.684  3.853]]\n",
    "db1 = [[0.    0.    1.674]]\n",
    "dA1 = [[22.629  6.897 35.788  6.744 42.924  6.35   7.172  6.037]]\n",
    "dW2.T = [[  0.      0.      0.   ]\n",
    " [  0.      0.    446.819]]\n",
    "db2 = [[0.    0.475]]\n",
    "dA2 = [[0.31  1.321 0.837]]`\n",
    "\n",
    "***\n",
    "\n",
    "Evaluate the following cells and observe the dropout effect on the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter optimisation using regularisation\n",
    "def dropout_model_fit(X, Y, n_h, dropouts, epochs, learning_rate, verbose=False, init=\"he\"):\n",
    "    \"\"\"\n",
    "    Optimise model parameters by performing gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    X -- n data samples  (n, n_x)\n",
    "    Y -- ground truth label vector of size (n, n_y)\n",
    "    n_h -- array with number of units in hidden layers, size K-1\n",
    "    dropouts -- optional argument, list of dropout rates (K-1)\n",
    "    epochs -- number of iteration updates through dataset\n",
    "    learning_rate -- learning rate of the gradient descent\n",
    "    init -- string, either 'random', 'zeros' or 'he'\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing model parameters\n",
    "    grads -- dictionary with final gradients\n",
    "    loss_log -- list of loss values for every 100 updates\n",
    "    \"\"\"\n",
    "    \n",
    "    # model topology, array [n_x, n_h[0], ..., n_h[K-1], n_y]\n",
    "    dims = t5.model_config(X, Y, n_h)\n",
    "\n",
    "    # create and initialise model parameters\n",
    "    if init == 'random':\n",
    "        params = random_init(dims)\n",
    "    elif init == 'zeros':\n",
    "        params = zeros_init(dims)\n",
    "    elif init == 'he':\n",
    "        params = he_init(dims)\n",
    "    \n",
    "    loss_log = []\n",
    "    K = len(params) >> 1\n",
    "    n = Y.shape[0]\n",
    "    for i in range(epochs):\n",
    "        A, loss, caches = forward_prop_with_dropout(params, X, Y, dropouts)\n",
    "        grads = back_prop_with_dropout(A, Y, caches, dropouts)\n",
    "        params = t5.update_params(params, grads, learning_rate)        \n",
    "        \n",
    "        # logs\n",
    "        if i % 100 == 0:\n",
    "            loss_log.append(loss.item())\n",
    "            if verbose:\n",
    "                print(\"Loss after {} epochs: {:.3f}\".format(i, loss))\n",
    "     \n",
    "    return params, grads, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "params, grads, loss_log = dropout_model_fit(X_train, Y_train, dropouts=[0.5, 0.3, 0.3], n_h=[18, 9, 9], epochs=30000, learning_rate=0.1, verbose=False, init='he')\n",
    "\n",
    "plt.plot(loss_log)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.show()\n",
    "\n",
    "print(\"loss after {} epochs = {:.2f}\".format(len(loss_log) *100, loss_log[-1]))\n",
    "Y_hat_train = t5.model_predict(params, X_train)\n",
    "Y_hat_test = t5.model_predict(params, X_test)\n",
    "train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
    "test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))\n",
    "print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "\n",
    "t5.plot_model(lambda x: t5.model_predict(params, x), X_test, Y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Overfitting is reduced and test accuracy improves with dropout. Dropout is a regularisation technique that works well in practice. \n",
    "\n",
    "**Note that a common mistake is to use dropout both in training and testing** when it should only be applied to forward and backward propagation in training. During training, the dropout activations must be scaled according to the dropout rates to ensure the expected value for the loss remains the same. The fact that we ignore dropout units means that the performance on the training set will be lower, however testing accuracy will improve.\n",
    "\n",
    "Both $L_2$ regularisation and dropout are effective regularisation techniques useful to combat overfitting, and keeping model parameters to small values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- EOF --"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
