{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "\n",
    "So far we have trained our model by updating its parameters using gradient descent. In this notebook we will study some more advanced optimisation strategies that can lead to better performance and faster training. The objective of an optimiser is to minimise the value of a loss function using an iterative algorithm. Optimising loss function with respect to the model parameters is rarely a convex problem and reaching a good local minimum can be challenging.\n",
    "\n",
    "> **Instructions:** ensure your Python environment is setup with the following additional packages: \n",
    "> \n",
    "> - `t6utils.py` contains unit tests to check your code, implementation for a simple deep feedforward neural network and some helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import t6utils as t6\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Mini-Batch (Stochastic) Gradient Descent Optimiser ##\n",
    "\n",
    "Gradient descent is a simple optimisation algorithm where a gradient step is taken with respect to all sample of the training set. It is sometimes referred to as **Batch Gradient Descent** (BGD or simply GD). This is the method we've been using until now.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 1:**</font> Complete the batch gradient descent update in the following function. Given a learning rate $\\lambda$, the update rule for the parameters of the $k$-th layer is given by:\n",
    ">\n",
    "> $$W^{[k]} = W^{[k]} - \\lambda \\text{ } \\frac{\\partial \\mathcal{L}}{\\partial W^{[k]}} = W^{[k]} - \\lambda \\text{ } dW^{[k]} \\tag{1}$$\n",
    "> $$b^{[k]} = b^{[k]} - \\lambda \\text{ } \\frac{\\partial \\mathcal{L}}{\\partial b^{[k]}} = b^{[k]} - \\lambda \\text{ } db^{[k]}\\tag{2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model parameters\n",
    "def gd_update_params(params, grads, learning_rate=0.8):\n",
    "    \"\"\"\n",
    "    Updates parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    grads -- dictionary containing gradients\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing updated parameters\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(params) >> 1\n",
    "    for k in range(1, K + 1):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        None \n",
    "        None \n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "params, grads = t6.update_params_test()\n",
    "params = gd_update_params(params, grads, 0.1)\n",
    "print(\"W1.T = {}\".format(params['W1'].T))\n",
    "print(\"b1 = {}\".format(params['b1']))\n",
    "print(\"W2.T = {}\".format(params['W2'].T))\n",
    "print(\"b2 = {}\".format(params['b2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `W1.T = [[-0.397  1.518  0.582  1.001]\n",
    " [ 0.778 -0.334  0.35  -1.558]\n",
    " [ 1.472  0.721 -0.231 -0.433]]\n",
    "b1 = [[-0.071 -0.686  0.24 ]]\n",
    "W2.T = [[-0.149  2.727  0.619]]\n",
    "b2 = [[0.748]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying gradient update for each sample in the training set defines a variant of the batch gradient descent called **Stochastic Gradient Descent** (SGD). This is equivalent to a mini-batch gradient descent where each mini-batch contains a single example. The update rule is the same for both SGD and BGD. \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"figs/bgd_optimisers.png\" style=\"width: 350px;\"/> </td>\n",
    "<td width=\"20px;\"></td>\n",
    "<td> <img src=\"figs/sgd_optimisers.png\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n",
    "<caption><center>**Figure 1:** Stochastic gradient descent (right) converges faster than batch gradient descent (left) but leads to many oscillations as it uses only one training example for every parameters update. </center></caption>\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "*** Mini-Batch Gradient Descent Optimiser ***\n",
    "\n",
    "In practice, we use a trade-off method called mini-batch gradient descent where parameters are updated from the gradients computed on a mini-batch of training examples e.g. 64.  \n",
    "\n",
    "For each epoch (full batch of examples), mini-batches are constructed by partitioning the shuffled training set. \n",
    "\n",
    "\n",
    "``` python\n",
    "# BGD optimisation\n",
    "for i in range(epochs):\n",
    "   A, loss, caches = forward_prop(params, X[j,:], Y[j,:])\n",
    "   grads = back_prop(A, Y[j,:], caches)\n",
    "   params = gd_update_params(params, grads, learning_rate)        \n",
    "\n",
    "# SGD optimisation    \n",
    "for i in range(epochs):\n",
    "    for j in range(X.shape[0]):\n",
    "        A, loss, caches = forward_prop(params, X[j,:], Y[j,:])\n",
    "        grads = back_prop(A, Y[j,:], caches)\n",
    "        params = gd_update_params(params, grads, learning_rate)        \n",
    "\n",
    "# Mini-batch GD optimisation    \n",
    "for i in range(epochs):\n",
    "    mini_bathes = stochastic_mini_batches(X, Y, mini_batch_sz, seed)\n",
    "    for j in range(len(mini_batches)):\n",
    "        (X_mini_bath, Y_mini_bath) = mini_batches[j]\n",
    "        A, loss, caches = forward_prop(params, X_mini_bath, Y_mini_bath)\n",
    "        grads = back_prop(A, Y_mini_bath, caches)\n",
    "        params = gd_update_params(params, grads, learning_rate)        \n",
    "```\n",
    "<listing><center>**Listing 1:** python code for various gradient optimisers</center></listing>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"figs/sgd_optimisers.png\" style=\"width: 350px;\"/> </td>\n",
    "<td width=\"20px;\"></td>\n",
    "<td> <img src=\"figs/mbgd_optimisers.png\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n",
    "<caption><center>**Figure 2:** Mini-batch gradient descent (right) is good compromise over stochastic gradient descent (left) with reduced oscillations and faster convergence.</center></caption>\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 2:**</font> Implement the  function `stochastic_mini_batches` to prepare mini-batches for the mini-batch optimiser by shuffling and partitioning the training set. Note that the last mini-batch may be smaller than the mini-batch size, $size_{mini-batch}$ if the total number of examples is not a multiple of $size_{mini-batch}$. In that case, the final mini-batch size will be $n-size_{mini-batch} \\times\\left\\lfloor \\frac{n}{size_{mini-batch}}\\right\\rfloor$ where $n$ is the total number of training examples (batch). \n",
    ">\n",
    "> Note that we often choose powers of two for the mini-batch size (e.g. 64, 128, 256).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and partition a training set into mini-batches\n",
    "def stochastic_mini_batches(X, Y, mini_batch_sz=128, seed=2019):\n",
    "    \"\"\"\n",
    "    Creates a list of random mini-batches\n",
    "    \n",
    "    Arguments:\n",
    "    X -- training set a numpy array of shape (n, n_x)\n",
    "    Y -- training ground truth vector of size (n, n_y)\n",
    "    mini_batch_sz -- size of the mini-batches, integer\n",
    "    seed -- random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of mini-batches [(X_mini_batch, Y_mini_batch), ..., (X_mini_batch, Y_mini_batch)]\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            \n",
    "    n = X.shape[0]\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: random permutation of (X, Y)\n",
    "    permutation = list(np.random.permutation(n))\n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    X_perm = None\n",
    "    Y_perm = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "\n",
    "\n",
    "    # Step 2: Partition\n",
    "    count = int(math.floor(n / mini_batch_sz))\n",
    "    for i in range(count):\n",
    "        ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "        X_mini_batch = None\n",
    "        Y_mini_batch = None\n",
    "        mini_batch = None\n",
    "        None\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    if n % mini_batch_sz != 0:\n",
    "        ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "        X_mini_batch = None\n",
    "        Y_mini_batch = None\n",
    "        mini_batch = None\n",
    "        None\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "X, Y, mini_batch_sz = t6.stochastic_mini_batch_test()\n",
    "mini_batches = stochastic_mini_batches(X, Y, mini_batch_sz)\n",
    "\n",
    "print(\"training set size = {}\".format(X.shape[0]))\n",
    "print(\"mini-batch size = {}\".format(mini_batch_sz))\n",
    "print(\"number of mini-batches = {}\".format(len(mini_batches)))\n",
    "\n",
    "import functools\n",
    "print(\"number of examples in all mini-batches = {}\".format(functools.reduce(lambda x, y : y[0].shape[0] + x , mini_batches, 0)))\n",
    "print(\"number of examples in last mini-batch = {}\".format(mini_batches[-1][0].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `training set size = 132\n",
    "mini-batch size = 64\n",
    "number of mini-batches = 3\n",
    "number of examples in all mini-batches = 132\n",
    "number of examples in last mini-batch = 4`\n",
    "\n",
    "## B. Gradient Descent Optimiser with Momentum ##\n",
    "\n",
    "Mini-batch gradient descent yields model parameter updates after seeing each set of mini-batch examples. As a result, the gradient direction has some relatively high variance, resulting in convergence oscillations. These can be reduced using momentum, taking into account past gradients to smooth out the update. \n",
    "\n",
    "As a result, a cache is required to hold the direction of previous gradients referred to with variable $v$. You can also think of $v$ as the velocity of a parameter ball rolling downhill and building up momentum (speed) in  the direction of the slope of the hill. A common strategy is to apply an exponentially decay on the past gradient average.  \n",
    "\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 3:**</font> Initialise the velocity as a python dictionary of arrays with zeros (same keys and shape as arrays in the `grads` dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise momentum parameters\n",
    "def initialise_velocity(params):\n",
    "    \"\"\"\n",
    "    Initialises the optimiser velocity\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    \n",
    "    Returns:\n",
    "    v -- dictionary containing current velocity\n",
    "        dW1 -- zero matrix of shape W1\n",
    "        db1 -- zero matrix of shape b1\n",
    "        ...\n",
    "        dWK -- zero matrix of shape WK\n",
    "        dbK -- zero matrix of shape WK\n",
    "    \"\"\"\n",
    "    \n",
    "    K = len(params) >> 1\n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for k in range(1, K + 1):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        v['dW{}'.format(k)] = None\n",
    "        v['db{}'.format(k)] = None\n",
    "        ### END OF YOUR CODE SEGMENT ###         \n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "params = t6.initialise_velocity_test()\n",
    "v = initialise_velocity(params)\n",
    "print(\"v[\\'dW1\\'] = {}\".format(v['dW1']))\n",
    "print(\"v[\\'db1\\'] = {}\".format(v['db1']))\n",
    "print(\"v[\\'dW2\\'].T = {}\".format(v['dW2'].T))\n",
    "print(\"v[\\'db2\\'] = {}\".format(v['db2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `v['dW1'] = [[0. 0. 0.]\n",
    " [0. 0. 0.]\n",
    " [0. 0. 0.]\n",
    " [0. 0. 0.]]\n",
    "v['db1'] = [[0. 0. 0.]]\n",
    "v['dW2'].T = [[0. 0. 0.]]\n",
    "v['db2'] = [[0.]]`\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 4:**</font> Implement the parameters update with momentum using the following rules:\n",
    ">\n",
    ">$$\\begin{cases}\n",
    "v_{dW^{[k]}} = \\beta v_{dW^{[k]}} + (1 - \\beta) dW^{[k]} \\\\\n",
    "v_{db^{[k]}} = \\beta v_{db^{[k]}} + (1 - \\beta) db^{[k]}\\\\\n",
    "W^{[k]} = W^{[k]} - \\lambda v_{dW^{[k]}}\\\\\n",
    "b^{[k]} = b^{[k]} - \\lambda v_{db^{[k]}} \n",
    "\\end{cases}\\notag$$\n",
    ">\n",
    "> where $k\\in 1\\ldots K$ refers to the $k$-th layer, $\\lambda$ is the learning rate and $\\beta$ is the momentum. All parameters are to be stored in the `params` dictionary. \n",
    ">\n",
    "> - Velocities are initialised to zeros and it will take a few iteration before the algorithm builds up some velocity momentum and take bigger steps in the gradient descent. When $\\beta = 0$, momentum gradient descent reduces to standard gradient descent. \n",
    "> - The larger the momentum $\\beta$, the smoother the update. However if $\\beta$ is too big, it may smooth out the updates too much. Common values for $\\beta$ range from 0.8 to 0.999. A reasonable default value is $\\beta = 0.9$. Tuning for the optimal $\\beta$ for your model requires that you test several values to determine what works best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model parameters using momentum\n",
    "def momentum_update_params(params, grads, v, beta=0.9, learning_rate=0.8):\n",
    "    \"\"\"\n",
    "    Updates parameters using gradient descent with momentum\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    grads -- dictionary containing gradients\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    v -- dictionary containing current velocity\n",
    "        dW1 -- momentum matrix of shape (n_x, n_h1)\n",
    "        db1 -- momentum vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- momentum matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- momentum vector of shape (1, n_y)\n",
    "    beta -- momentum scalar (hyperparameter)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing updated parameters\n",
    "    v -- dictionary containing updated velocities\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(params) >> 1\n",
    "    \n",
    "    for k in range(1, K + 1):\n",
    "        ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "        v['dW{}'.format(k)] = None\n",
    "        v['db{}'.format(k)] = None\n",
    "        params['W{}'.format(k)] = None\n",
    "        params['b{}'.format(k)] = None \n",
    "        ### END OF YOUR CODE SEGMENT ###         \n",
    "    \n",
    "    return params, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads, v = t6.momentum_update_params_test()\n",
    "params, v = momentum_update_params(params, grads, v, beta=0.9, learning_rate=0.01)\n",
    "print(\"W1 = {}\".format(params['W1']))\n",
    "print(\"b1 = {}\".format(params['b1']))\n",
    "print(\"W2.T = {}\".format(params['W2'].T))\n",
    "print(\"b2 = {}\".format(params['b2']))\n",
    "print(\"v[\\'dW1\\'] = {}\".format(v['dW1']))\n",
    "print(\"v[\\'db1\\'] = {}\".format(v['db1']))\n",
    "print(\"v[\\'dW2\\'].T = {}\".format(v['dW2'].T))\n",
    "print(\"v[\\'db2\\'] = {}\".format(v['db2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `W1 = [[-0.219  0.821  1.481]\n",
    " [ 1.334 -0.362  0.686]\n",
    " [ 0.574  0.288 -0.236]\n",
    " [ 0.954 -1.688 -0.346]]\n",
    "b1 = [[ 0.016 -0.517  0.244]]\n",
    "W2.T = [[-0.189  2.673  0.466]]\n",
    "b2 = [[0.845]]\n",
    "v['dW1'] = [[ 0.179  0.044  0.01 ]\n",
    " [-0.186 -0.028 -0.035]\n",
    " [-0.008 -0.063 -0.004]\n",
    " [-0.048 -0.131  0.088]]\n",
    "v['db1'] = [[0.088 0.171 0.005]]\n",
    "v['dW2'].T = [[-0.04  -0.055 -0.155]]\n",
    "v['db2'] = [[0.098]]`\n",
    "\n",
    "## C. ADAM Optimiser ##\n",
    "\n",
    "Adam is one of the most popular optimiser used for training neural networks, combining ideas from RMSProp and momentum. It yields effective training, evaluating the following quantities to perform the parameters updates:\n",
    "\n",
    "- exponentially weighted average of past gradients stored in $v$ and $\\bar{v}$ (with bias correction) \n",
    "- exponentially weighted average of the squares of the past gradients stored $s$ and $\\bar{s}$ (with bias correction)\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 5:**</font> Initialize the ADAM variables $v, s$ as a python dictionary of arrays with zeros (same keys and shape as arrays in the `grads` dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise adam parameters\n",
    "def initialise_moments(params):\n",
    "    \"\"\"\n",
    "    Initialises the optimiser 1st and 2nd moments\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    \n",
    "    Returns:\n",
    "    v -- dictionary containing current 1st moment estimates\n",
    "        dW1 -- zero matrix of shape W1\n",
    "        db1 -- zero matrix of shape b1\n",
    "        ...\n",
    "        dWK -- zero matrix of shape WK\n",
    "        dbK -- zero matrix of shape WK\n",
    "    s -- dictionary containing current 2nd moment estimates\n",
    "        dW1 -- zero matrix of shape W1\n",
    "        db1 -- zero matrix of shape b1\n",
    "        ...\n",
    "        dWK -- zero matrix of shape WK\n",
    "        dbK -- zero matrix of shape WK\n",
    "    \"\"\"\n",
    "    \n",
    "    K = len(params) >> 1\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize velocities\n",
    "    for k in range(1, K + 1):\n",
    "        ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "        v['dW{}'.format(k)] = None\n",
    "        v['db{}'.format(k)] = None\n",
    "        s['dW{}'.format(k)] = None\n",
    "        s['db{}'.format(k)] = None\n",
    "        ### END OF YOUR CODE SEGMENT ###         \n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "params = t6.initialise_velocity_test()\n",
    "v, s = initialise_moments(params)\n",
    "print(\"v[\\'dW1\\'] = {}\".format(v['dW1']))\n",
    "print(\"v[\\'db1\\'] = {}\".format(v['db1']))\n",
    "print(\"v[\\'dW2\\'].T = {}\".format(v['dW2'].T))\n",
    "print(\"v[\\'db2\\'] = {}\".format(v['db2']))\n",
    "print(\"s[\\'dW1\\'] = {}\".format(s['dW1']))\n",
    "print(\"s[\\'db1\\'] = {}\".format(s['db1']))\n",
    "print(\"s[\\'dW2\\'].T = {}\".format(s['dW2'].T))\n",
    "print(\"s[\\'db2\\'] = {}\".format(s['db2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `v['dW1'] = [[0. 0. 0.]\n",
    " [0. 0. 0.]\n",
    " [0. 0. 0.]\n",
    " [0. 0. 0.]]\n",
    "v['db1'] = [[0. 0. 0.]]\n",
    "v['dW2'].T = [[0. 0. 0.]]\n",
    "v['db2'] = [[0.]]\n",
    "s['dW1'] = [[0. 0. 0.]\n",
    " [0. 0. 0.]\n",
    " [0. 0. 0.]\n",
    " [0. 0. 0.]]\n",
    "s['db1'] = [[0. 0. 0.]]\n",
    "s['dW2'].T = [[0. 0. 0.]]\n",
    "s['db2'] = [[0.]]`\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 6:**</font> Implement the parameters update with adam using the following rules:\n",
    ">\n",
    "> $$\\begin{cases}\n",
    "v_{dW^{[k]}} = \\beta_1 v_{dW^{[k]}} + (1 - \\beta_1) dW^{[k]}\\\\%\\frac{\\partial \\mathcal{L} }{ \\partial W^{[k]} } \\\\\n",
    "\\bar{v}_{dW^{[k]}} = \\frac{v_{dW^{[k]}}}{1 - \\beta_1^t} \\\\\n",
    "s_{dW^{[k]}} = \\beta_2 s_{dW^{[k]}} + (1 - \\beta_2) (dW^{[k]})^2\\\\%\\left(\\frac{\\partial \\mathcal{L} }{\\partial W^{[k]} }\\right)^2 \\\\\n",
    "\\bar{s}_{dW^{[k]}} = \\frac{s_{dW^{[k]}}}{1 - \\beta_2^t} \\\\\n",
    "W^{[k]} = W^{[k]} - \\lambda \\frac{\\bar{v}_{dW^{[k]}}}{\\sqrt{\\bar{s}_{dW^{[k]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    ">\n",
    "> $$\\begin{cases}\n",
    "v_{db^{[k]}} = \\beta_1 v_{db^{[k]}} + (1 - \\beta_1) db^{[k]}\\\\\n",
    "\\bar{v}_{db^{[k]}} = \\frac{v_{db^{[k]}}}{1 - \\beta_1^t} \\\\\n",
    "s_{db^{[k]}} = \\beta_2 s_{db^{[k]}} + (1 - \\beta_2) (db^{[k]})^2\\\\\n",
    "\\bar{s}_{db^{[k]}} = \\frac{s_{db^{[k]}}}{1 - \\beta_2^t} \\\\\n",
    "b^{[k]} = b^{[k]} - \\lambda \\frac{\\bar{v}_{db^{[k]}}}{\\sqrt{\\bar{s}_{db^{[k]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    ">\n",
    "> where $k\\in 1\\ldots K$ refers to the $k$-th layer, $t$ is the number of update steps considered in the ADAM algorithm, $\\beta_1$ and $\\beta_2$ are the two hyper-parameters controlling the exponential decays, $\\lambda$ is the learning rate and $\\varepsilon$ is a small scalar to prevent numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model parameters using ADAM\n",
    "def adam_update_params(params, grads, v, s, t, beta1=0.9, beta2=0.999, epsilon=1e-8, learning_rate=0.8):\n",
    "    \"\"\"\n",
    "    Updates parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    grads -- dictionary containing gradients\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    v -- dictionary containing current 1st moment estimates\n",
    "        same keys as grads\n",
    "    s -- dictionary containing current 2nd moment estimates\n",
    "        same keys as grads\n",
    "    t -- parameter update counter (integer)\n",
    "    beta1 -- 1st moment estimate scalar (hyperparameter)\n",
    "    beta2 -- 2nd moment estimate scalar (hyperparameter)\n",
    "    epsilon -- small scalar for numerical stability (hyperparameter)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing updated parameters\n",
    "    v -- dictionary containing updated 1st moment estimates\n",
    "    s -- dictionary containing updated 2nd moment estimates\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(params) >> 1\n",
    "    v_bar = {}\n",
    "    s_bar = {}\n",
    "    \n",
    "    for k in range(1, K + 1):\n",
    "        ### INPUT YOUR CODE HERE ### (10 lines)\n",
    "        v['dW{}'.format(k)] = None\n",
    "        v['db{}'.format(k)] = None\n",
    "\n",
    "        s['dW{}'.format(k)] = None\n",
    "        s['db{}'.format(k)] = None\n",
    "\n",
    "        v_bar['dW{}'.format(k)] = None\n",
    "        v_bar['db{}'.format(k)] = None\n",
    "\n",
    "        s_bar['dW{}'.format(k)] = None\n",
    "        s_bar['db{}'.format(k)] = None\n",
    "        \n",
    "        params['W{}'.format(k)] = None\n",
    "        params['b{}'.format(k)] = None\n",
    "        ### END OF YOUR CODE SEGMENT ###         \n",
    "    \n",
    "    return params, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "params, grads, v, s = t6.adam_update_params_test()\n",
    "params, v, s = adam_update_params(params, grads, v, s, t=3, beta1=0.9, beta2=0.999, epsilon=1e-8, learning_rate=0.01)\n",
    "print(\"W1 = {}\".format(params['W1']))\n",
    "print(\"b1 = {}\".format(params['b1']))\n",
    "print(\"W2.T = {}\".format(params['W2'].T))\n",
    "print(\"b2 = {}\".format(params['b2']))\n",
    "print(\"v[\\'dW1\\'] = {}\".format(v['dW1']))\n",
    "print(\"v[\\'db1\\'] = {}\".format(v['db1']))\n",
    "print(\"v[\\'dW2\\'].T = {}\".format(v['dW2'].T))\n",
    "print(\"v[\\'db2\\'] = {}\".format(v['db2']))\n",
    "print(\"s[\\'dW1\\'] = {}\".format(s['dW1']))\n",
    "print(\"s[\\'db1\\'] = {}\".format(s['db1']))\n",
    "print(\"s[\\'dW2\\'].T = {}\".format(s['dW2'].T))\n",
    "print(\"s[\\'db2\\'] = {}\".format(s['db2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `W1 = [[-0.224  0.815  1.475]\n",
    " [ 1.338 -0.355  0.692]\n",
    " [ 0.58   0.294 -0.229]\n",
    " [ 0.96  -1.683 -0.351]]\n",
    "b1 = [[ 0.011 -0.521  0.238]]\n",
    "W2.T = [[-0.183  2.679  0.471]]\n",
    "b2 = [[0.84]]\n",
    "v['dW1'] = [[ 0.179  0.044  0.01 ]\n",
    " [-0.186 -0.028 -0.035]\n",
    " [-0.008 -0.063 -0.004]\n",
    " [-0.048 -0.131  0.088]]\n",
    "v['db1'] = [[0.088 0.171 0.005]]\n",
    "v['dW2'].T = [[-0.04  -0.055 -0.155]]\n",
    "v['db2'] = [[0.098]]\n",
    "s['dW1'] = [[0.003 0.    0.   ]\n",
    " [0.003 0.    0.   ]\n",
    " [0.    0.    0.   ]\n",
    " [0.    0.002 0.001]]\n",
    "s['db1'] = [[0.001 0.003 0.   ]]\n",
    "s['dW2'].T = [[0.    0.    0.002]]\n",
    "s['db2'] = [[0.001]]`\n",
    "\n",
    "***\n",
    "\n",
    "## D. Evaluation ##\n",
    "\n",
    "We have three gradient descent optimisers in the form of mini-batch gradient descent, momentum and adam. We'll now evaluate their respective performance on a planar dataset using our multi-layer feed-forward model.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 7:**</font> Complete the following function to setup your model and training algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter optimisation using different optimisers\n",
    "def model_fit(X, Y, n_h=[20, 8], optimiser=None, epochs=15000, learning_rate=0.01, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimise model parameters by performing gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    X -- n data samples  (n, n_x)\n",
    "    Y -- ground truth label vector of size (n, n_y)\n",
    "    n_h -- array with number of units in hidden layers, size K-1\n",
    "    optimiser -- dictionary\n",
    "        name: string, optimiser name 'gradient_descent', 'momentum' or 'adam'\n",
    "        mini_batch_sz: int, size of mini-batch\n",
    "        beta: scalar, required hyperparameter for momentum \n",
    "        beta1: scalar, required hyperparameter for adam \n",
    "        beta2: scalar, required hyperparameter for adam \n",
    "        epsilon: scalar, required hyperparameter for adam \n",
    "    epochs -- number of iteration updates through dataset\n",
    "    learning_rate -- learning rate of the gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing model parameters\n",
    "    grads -- dictionary with final gradients\n",
    "    loss_log -- list of loss values for every 100 updates\n",
    "    \"\"\"\n",
    "    \n",
    "    dims = t6.model_config(X, Y, n_h)\n",
    "    seed = 2019 # for reproducibility\n",
    "    params = t6.he_init(dims) # initialise model parameters\n",
    "    loss_log = []\n",
    "    \n",
    "    # initialise optimiser\n",
    "    mini_batch_sz = optimiser['mini_batch_sz']\n",
    "    if optimiser['name'] == 'momentum':\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        v = None\n",
    "        beta = None\n",
    "        ### END OF YOUR CODE SEGMENT ###         \n",
    "    elif optimiser['name'] == 'adam':\n",
    "        ### INPUT YOUR CODE HERE ### (5 lines)\n",
    "        v, s = None\n",
    "        beta1 = None\n",
    "        beta2 = None\n",
    "        epsilon = None\n",
    "        t = None\n",
    "        ### END OF YOUR CODE SEGMENT ###         \n",
    "\n",
    "    for i in range(epochs):\n",
    "        mini_batches = stochastic_mini_batches(X, Y, mini_batch_sz, seed)\n",
    "        seed = seed + 1\n",
    "        for j in range(len(mini_batches)):\n",
    "            (X_mini_batch, Y_mini_batch) = mini_batches[j]\n",
    "            \n",
    "            A, loss, caches = t6.forward_prop(params, X_mini_batch, Y_mini_batch) # Cost and gradient computation\n",
    "            grads = t6.back_prop(A, Y_mini_batch, caches) # Backprop\n",
    "            \n",
    "            # parameter update\n",
    "            if optimiser['name'] == 'momentum':\n",
    "                ### INPUT YOUR CODE HERE ### (1 line)\n",
    "                None # Momentum\n",
    "                ### END OF YOUR CODE SEGMENT ###         \n",
    "            elif optimiser['name'] == 'adam':\n",
    "                ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "                t = None\n",
    "                None # Adam\n",
    "                ### END OF YOUR CODE SEGMENT ###         \n",
    "            else:\n",
    "                ### INPUT YOUR CODE HERE ### (1 lines)\n",
    "                None # Gradient descent\n",
    "                ### END OF YOUR CODE SEGMENT ###         \n",
    "            \n",
    "        # logs\n",
    "        if i % 100 == 0:\n",
    "            loss_log.append(loss.item())\n",
    "        if verbose and (i == 0 or i % 1000 == 999):\n",
    "            print(\"Loss after {} epoch{}: {:.5f}\".format(i + 1, 's' if i > 0 else '', loss))\n",
    "     \n",
    "    return params, grads, loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the following set to create a simple dataset from Scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "\n",
    "# training set\n",
    "X_train, Y_train = sklearn.datasets.make_moons(n_samples=512, noise=.2)\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "# test set\n",
    "X_test, Y_test = sklearn.datasets.make_moons(n_samples=256, noise=.2)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train.reshape(-1), s=20, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the following cell to optimise the model parameters using mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train feed-forward model using mini-batch gradient descent\n",
    "optimiser = {'name': 'gradient_descent', 'mini_batch_sz': 32}\n",
    "lambd = 1e-4\n",
    "params, grads, loss_log = model_fit(X_train, Y_train, n_h=[8, 3], optimiser=optimiser, epochs=10000, learning_rate=lambd)\n",
    "\n",
    "# evaluate model\n",
    "Y_hat_train = t6.model_predict(params, X_train)\n",
    "Y_hat_test = t6.model_predict(params, X_test)\n",
    "train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
    "test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))\n",
    "print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "\n",
    "t6.plot_model(lambda x: t6.model_predict(params, x), X_test, Y_test.reshape(-1), \"Optimiser: mini-batch gradient descent\")\n",
    "\n",
    "# plot loss\n",
    "plt.plot(loss_log)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.title(\"Learning rate = {}\".format(lambd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the following cell to optimise the model parameters using momentum. Gains are negligible with this shallow model but will be more noticeable with larger models. Note the oscillations during training with some mini-batches leading to more stochastic noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train feed-forward model using momentum\n",
    "optimiser = {'name': 'momentum', 'mini_batch_sz': 32, 'beta': 0.9}\n",
    "lambd = 1e-4\n",
    "params, grads, loss_log = model_fit(X_train, Y_train, n_h=[8, 3], optimiser=optimiser, epochs=10000, learning_rate=lambd)\n",
    "\n",
    "# evaluate model\n",
    "Y_hat_train = t6.model_predict(params, X_train)\n",
    "Y_hat_test = t6.model_predict(params, X_test)\n",
    "train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
    "test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))\n",
    "print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "\n",
    "t6.plot_model(lambda x: t6.model_predict(params, x), X_test, Y_test.reshape(-1), \"Optimiser: momentum\")\n",
    "\n",
    "# plot loss\n",
    "plt.plot(loss_log)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.title(\"Learning rate = {}\".format(lambd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the following cell to optimise the model parameters using Adam. Note the faster convergence of the Adam algorithm. Both gradient descent and momentum will eventually reach similar performance if trained longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train feed-forward model using adam\n",
    "optimiser = {'name': 'adam', 'mini_batch_sz': 32, 'beta1': 0.9, 'beta2': 0.999, 'epsilon':1e-8}\n",
    "lambd = 1e-4\n",
    "params, grads, loss_log = model_fit(X_train, Y_train, n_h=[8, 3], optimiser=optimiser, epochs=10000, learning_rate=lambd)\n",
    "\n",
    "# evaluate model\n",
    "Y_hat_train = t6.model_predict(params, X_train)\n",
    "Y_hat_test = t6.model_predict(params, X_test)\n",
    "train_acc = 100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))\n",
    "test_acc = 100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))\n",
    "print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "\n",
    "t6.plot_model(lambda x: t6.model_predict(params, x), X_test, Y_test.reshape(-1), \"Optimiser: Adam\")\n",
    "\n",
    "# plot loss\n",
    "plt.plot(loss_log)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.title(\"Learning rate = {}\".format(lambd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adam optimiser (https://arxiv.org/pdf/1412.6980.pdf) is a popular algorithm for training neural networks. It generally achieves good optimisation with default hyper-parameter values $\\beta_1$ and $\\beta_2$ and a little tuning of the learning rate $\\lambda$. Note the additional memory requirements for training ; in addition to the gradient, two momentum scalars are cached for each trainable model parameter.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"figs/opt1.gif\" style=\"width: 350px;\"/> </td>\n",
    "<td width=\"20px;\"></td>\n",
    "<td> <img src=\"figs/opt2.gif\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n",
    "<caption><center>**Figure 3:** Optimisation behaviour for a loss function around saddle point (left). Illustration of oscillations and resulting overshooting (right). (Illustration A. Radford)\n",
    " </center></caption>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- EOF --"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
